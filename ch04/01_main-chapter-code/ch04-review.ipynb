{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "torch version: 2.4.1\n",
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPT Config:\n",
    "- Vocabulary size: 50,257\n",
    "- Context length: 1024\n",
    "- Embedding dimensions: 768\n",
    "- Number of attention heads: 12\n",
    "- Number of transformer blocks: 12\n",
    "- Dropout rate\n",
    "- QKV Bias (true/false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    'vocab_size': 50_257,\n",
    "    'context_length': 1_024,\n",
    "    'emb_dim': 768,\n",
    "    'n_heads': 12,\n",
    "    'n_trans': 12,\n",
    "    'drop_rate': 0.1,\n",
    "    'qkv_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GPT Model.\n",
    "\n",
    "Layers:\n",
    "- Embedding x 1\n",
    "- Positional Encoding x 1\n",
    "- Dropout x 1\n",
    "- Transformer blocks x 12\n",
    "- Normalization x 1\n",
    "- Linear layer x 1\n",
    "\n",
    "Input shape: (n_sentence, n_tokens)\n",
    "\n",
    "Output: (n_sentence, n_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyNorm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos = nn.Embedding(cfg['context_length'],     cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "        self.transform = nn.ModuleList([\n",
    "            DummyTransformer() for _ in range(cfg['n_trans'])\n",
    "        ])\n",
    "        self.norm = DummyNorm()\n",
    "        self.linear = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], cfg['qkv_bias'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        x = self.emb(x)\n",
    "\n",
    "        pos_idx = torch.arange(seq_len, device = x.device)\n",
    "        pos_emb = self.pos(pos_idx)\n",
    "\n",
    "        x = x + pos_emb\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for trf in self.transform:\n",
    "            x = trf(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given 2 sentences of `cfg['seq_len']` tokens each:\n",
    "- Tokenize them using `tiktoken`'s `gpt2` tokenizer\n",
    "- Create a tensor of size `(2, sequence length, gpt2's vocab size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "texts = [\n",
    "    'Every effort moves you',\n",
    "    'Every day holds a'\n",
    "]\n",
    "\n",
    "input = torch.tensor([\n",
    "    tokenizer.encode(text) for text in texts\n",
    "])\n",
    "\n",
    "print(input)\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyGPTModel(\n",
       "  (emb): Embedding(50257, 768)\n",
       "  (pos): Embedding(1024, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transform): ModuleList(\n",
       "    (0-11): 12 x DummyTransformer()\n",
       "  )\n",
       "  (norm): DummyNorm()\n",
       "  (linear): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.1947,  0.1392, -0.8616,  ..., -1.4987, -0.0314, -0.4490],\n",
      "         [ 0.0497,  0.3861, -0.3281,  ..., -0.1826,  1.3084,  0.9867],\n",
      "         [ 0.7005,  1.4747, -0.4149,  ...,  1.7756, -0.2280,  0.5384],\n",
      "         [ 0.4885,  1.7545, -0.6707,  ...,  1.1501, -0.1143, -0.9368]],\n",
      "\n",
      "        [[-1.1947,  0.1392, -0.8616,  ..., -1.4987, -0.0314, -0.4490],\n",
      "         [-0.5591,  0.5797, -0.1296,  ...,  0.2691,  0.3151,  1.4046],\n",
      "         [ 0.8524,  1.2833, -0.1786,  ..., -0.1982,  0.1097,  0.2812],\n",
      "         [-0.0190, -0.8277,  0.2299,  ...,  1.7974, -0.1646, -0.1049]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Normalization layer with: epsilon, scale and shift matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 20])\n"
     ]
    }
   ],
   "source": [
    "norm_input = torch.randn(4, 20) # sample sentence of 4 tokens each with 20 dimensions\n",
    "print(norm_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([[    -0.0000],\n",
      "        [    -0.0000],\n",
      "        [    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "norm_layer = LayerNorm(emb_dim=20)\n",
    "norm_output = norm_layer(norm_input)\n",
    "\n",
    "mean = norm_output.mean(dim=-1, keepdim=True)\n",
    "var = norm_output.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "print('Mean:', mean)\n",
    "print('Variance:', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement FeedForward module that applies these layers sequentially:\n",
    "- Linear emb_dim, 4 x emb_dim\n",
    "- GELU\n",
    "- Linear 4 x emb_dim, emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 4 * emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = FeedForward(GPT_CONFIG_124M['emb_dim'])\n",
    "ff.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer block:\n",
    "- Layer norm 1\n",
    "- Masked multi head attention\n",
    "- Dropout\n",
    "- Shortcut\n",
    "- Layer norm 2\n",
    "- Feed forward\n",
    "- Dropout\n",
    "- Shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,  emb_dim, n_heads, drop_rate) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = nn.MultiheadAttention(emb_dim, n_heads, dropout=drop_rate)\n",
    "        self.dropout1 = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.ff = FeedForward(emb_dim)\n",
    "        self.dropout2 = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_out, _ = self.mha(norm_x, norm_x, norm_x)\n",
    "        attn_out = self.dropout1(attn_out)\n",
    "        x = x + attn_out\n",
    "\n",
    "        norm_x = self.norm2(x)\n",
    "        ff_out = self.ff(norm_x)\n",
    "        ff_out = self.dropout2(ff_out)\n",
    "        x = x + ff_out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = Transformer(GPT_CONFIG_124M['emb_dim'], GPT_CONFIG_124M['n_heads'], GPT_CONFIG_124M['drop_rate'])\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos = nn.Embedding(cfg['context_length'],     cfg['emb_dim'])\n",
    "        self.dropout = nn.Dropout(cfg['drop_rate'])\n",
    "        self.transform = nn.ModuleList([\n",
    "            Transformer(cfg['emb_dim'], cfg['n_heads'], cfg['drop_rate']) for _ in range(cfg['n_trans'])\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.linear = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], cfg['qkv_bias'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        x = self.emb(x)\n",
    "\n",
    "        pos_idx = torch.arange(seq_len, device = x.device)\n",
    "        pos_emb = self.pos(pos_idx)\n",
    "\n",
    "        x = x + pos_emb\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for trf in self.transform:\n",
    "            x = trf(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "output = model(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        # truncate the input to fit context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input, n_tokens):\n",
    "    input_tensor = torch.tensor(tokenizer.encode(input)).unsqueeze(0)\n",
    "    output_tensor = generate_text_simple(model, input_tensor, n_tokens, GPT_CONFIG_124M['context_length'])\n",
    "    decoded_text = tokenizer.decode(output_tensor.squeeze(0).tolist())\n",
    "    print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell nah590 Citadel CoastAbsolutelyresources Franco proper Explosive Germ Petty\n"
     ]
    }
   ],
   "source": [
    "generate_text('hell nah', 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
